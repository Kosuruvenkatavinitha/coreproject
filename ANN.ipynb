{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Z_KKoxPdAYIQAiHhaooKFHtf5wOU9_0T",
      "authorship_tag": "ABX9TyN2oq8py5Jq5/XPH8A7jGCF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kosuruvenkatavinitha/coreproject/blob/main/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlK2qQKRz0Fy",
        "outputId": "58acd727-7b80-4bae-ac3f-aac47eaf401c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 1s 1ms/step - loss: 0.2518 - accuracy: 0.9595\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9653\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.9660\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9682\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9696\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9705\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9705\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9703\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0871 - accuracy: 0.9710\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9714\n",
            "63/63 [==============================] - 0s 792us/step\n",
            "Anomaly Detection Results:\n",
            "Accuracy: 0.8965\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94      1939\n",
            "           1       0.14      0.44      0.21        61\n",
            "\n",
            "    accuracy                           0.90      2000\n",
            "   macro avg       0.56      0.68      0.58      2000\n",
            "weighted avg       0.96      0.90      0.92      2000\n",
            "\n",
            "\n",
            "ANN Results:\n",
            "Accuracy: 0.9735\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1939\n",
            "           1       0.65      0.28      0.39        61\n",
            "\n",
            "    accuracy                           0.97      2000\n",
            "   macro avg       0.82      0.64      0.69      2000\n",
            "weighted avg       0.97      0.97      0.97      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load your CSV dataset\n",
        "# Assuming you have a 'label' column indicating normal (0) or anomaly (1)\n",
        "df = pd.read_csv('/content/predictive_maintenance.csv')\n",
        "\n",
        "# Drop non-numeric columns for simplicity\n",
        "df_numeric = df.drop(['Product ID', 'Type', 'Failure Type'], axis=1)\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "X = df_numeric.iloc[:, :-1].values\n",
        "y = df_numeric.iloc[:, -1].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for ANN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Anomaly Detection with Isolation Forest\n",
        "clf_anomaly = IsolationForest(contamination=0.1, random_state=42)\n",
        "y_anomaly_pred = clf_anomaly.fit_predict(X_test_scaled)\n",
        "\n",
        "# Convert predictions to 0 for normal and 1 for anomaly\n",
        "y_anomaly_pred = [1 if x == -1 else 0 for x in y_anomaly_pred]\n",
        "\n",
        "# Define and train the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Predictions with ANN\n",
        "y_ann_pred = np.round(model.predict(X_test_scaled))\n",
        "\n",
        "# Evaluate Anomaly Detection\n",
        "print(\"Anomaly Detection Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_anomaly_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_anomaly_pred))\n",
        "\n",
        "# Evaluate ANN\n",
        "print(\"\\nANN Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_ann_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_ann_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICWhzjXZ9OdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZ80Pwna-D8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}